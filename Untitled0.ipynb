{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#======================================================================\n",
        "#  Exercise:  1.Tokenization\n",
        "#             2.Stopword Removal\n",
        "#             3.Stemming and Lemmetization\n",
        "#             4.Part-of-Speech(POS) Tagging\n",
        "#             5.Named Entity Recognition\n",
        "#Author: Amruth Gadepalli\n",
        "#IIIT Banglore.2nd Year\n",
        "#\n",
        "#Date : 6th January 2024\n",
        "#======================================================================"
      ],
      "metadata": {
        "id": "ZfHDkp3ZqmvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITWywWXbC_fv",
        "outputId": "0ba726f7-6046-4f30-ff57-bfb1f237ada6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "4c4IfzTyDSh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fKs8Ku1Dn_v",
        "outputId": "c2d08ab2-c995-4565-917f-d8134594ad8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I have imported all the necessary libraries.\n"
      ],
      "metadata": {
        "id": "ygtceAsSD7wM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1:\n"
      ],
      "metadata": {
        "id": "ziMRJ72aEPUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"this is a sample text that will be broken down into smaller tokens.\"\n",
        "words = word_tokenize(text)\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"words:\",  words)\n",
        "print(\"sentences:\", sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wv28-tZEl5i",
        "outputId": "21130cfc-a10f-40cf-9598-f402de959755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words: ['this', 'is', 'a', 'sample', 'text', 'that', 'will', 'be', 'broken', 'down', 'into', 'smaller', 'tokens', '.']\n",
            "sentences: ['this is a sample text that will be broken down into smaller tokens.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can observe that the punctuation is also being taken as seperate words."
      ],
      "metadata": {
        "id": "FcW5p_ZhFsKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercise 2:"
      ],
      "metadata": {
        "id": "dxmHOW_EF2be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "#here, i am removing stopwords from the previous word tokens that i got from text data sample.\n",
        "\n",
        "print(\"Filtered Words: \", filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoJ_H5d8GOl2",
        "outputId": "9ed42afa-c8e9-4f98-b984-a0d5bb9e3479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words:  ['sample', 'text', 'broken', 'smaller', 'tokens', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3"
      ],
      "metadata": {
        "id": "4T5Wa8EdIWiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"writing\",\"hitting\",\"flying\",\"ran\",\"burnt\"]\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "print(\"Stemmed Words:\",stemmed_words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1DzZBZ7I28v",
        "outputId": "b011dcb7-08ec-4ab9-bdb5-ad4366670d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['write', 'hit', 'fli', 'ran', 'burnt']\n",
            "Lemmatized words: ['write', 'hit', 'fly', 'run', 'burn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the reason we got 'fli' in stemmed words and 'fly' in lemmatized words is because, stemming reduces the word to its root form some times resulting in non-dictionary words. whereas in lemmatizing , we reduce the words to dictionary or base form.\n",
        "\n",
        "the choice of the process depends on the task in hand."
      ],
      "metadata": {
        "id": "ZVdzZHdgK_RH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4:"
      ],
      "metadata": {
        "id": "euGdc43MLlOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the required NLTK resource\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsdE3jJlL8vD",
        "outputId": "cbd3e4b8-ec16-4377-ed05-8a76badbaf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "tagged_words = pos_tag(words)\n",
        "\n",
        "print(\"POS tagging:\", tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyVGmLqaNFg9",
        "outputId": "3722edf0-4c30-4728-867c-322c9eae3f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging: [('writing', 'VBG'), ('hitting', 'VBG'), ('flying', 'VBG'), ('ran', 'NN'), ('burnt', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl2Uw2VNOhBI",
        "outputId": "63298701-f971-4742-fcc1-2faf33aa135b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5\n",
        "\n"
      ],
      "metadata": {
        "id": "XbWOUg4BNdcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y93NZ3hpO5pP",
        "outputId": "56ac3cb9-4a1d-4942-8570-75e47e6473e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "sentence = \"Narendra Modi is from India.\"\n",
        "\n",
        "tagged_sentence = pos_tag(word_tokenize(sentence))\n",
        "named_entities = ne_chunk(tagged_sentence)\n",
        "\n",
        "print(\"Named Entities:\", named_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjuvGNG0N0ER",
        "outputId": "3b395d7d-7ccd-46a8-805c-6051010d0026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: (S\n",
            "  (PERSON Narendra/NNP)\n",
            "  (ORGANIZATION Modi/NNP)\n",
            "  is/VBZ\n",
            "  from/IN\n",
            "  (GPE India/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N4_VWRoHNc-i"
      }
    }
  ]
}